{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import dtypes\n",
    "from solarimage.classifier.nn.trainer import TrainIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting example/MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting example/MNIST_data/train-labels-idx1-ubyte.gz\nExtracting example/MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting example/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"example/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainIter(32, 1000, \n",
    "                    images=mnist.train.images, \n",
    "                    labels=mnist.train.labels, \n",
    "                    sample_size=64,\n",
    "                    hidden_size=128,\n",
    "                    learning_rate=1e-3,\n",
    "                    reshape=False,\n",
    "                    dtype=dtypes.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_loss_list, g_loss_list = trainer.run_genradvers()\n",
    "# z, theta_g, x, theta_d = trainer.get_genradvers_var()\n",
    "# d_loss, g_loss = trainer.get_genradvers_loss(z, theta_g, x, theta_d, trainer.batch_size)\n",
    "# d_solver, g_solver = trainer.get_genradvers_solver(d_loss, theta_d, g_loss, theta_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.9979839,\n 3.5998471,\n 3.5669134,\n 3.5909932,\n 3.533967,\n 3.532711,\n 3.5099607,\n 3.5080726,\n 3.5577197,\n 3.5515223,\n 3.5067558,\n 3.5397043,\n 3.5213306,\n 3.5340202,\n 3.52196,\n 3.5265362,\n 3.5209658,\n 3.5088415,\n 3.5027642,\n 3.5010417,\n 3.4866295,\n 3.5077255,\n 3.4960368,\n 3.4999113,\n 3.4947238,\n 3.4999657,\n 3.4951181,\n 3.4938231,\n 3.5074492,\n 3.4929857,\n 3.49224,\n 3.5023463,\n 3.5139096,\n 3.5018315,\n 3.5053039,\n 3.5030847,\n 3.4939525,\n 3.4885604,\n 3.4912405,\n 3.4873075,\n 3.4888623,\n 3.4861467,\n 3.4874027,\n 3.5020819,\n 3.4857445,\n 3.4970298,\n 3.4923105,\n 3.4867666,\n 3.4845264,\n 3.4852514,\n 3.4837117,\n 3.4783864,\n 3.4794867,\n 3.4840817,\n 3.4887333,\n 3.4808717,\n 3.4813383,\n 3.4847317,\n 3.4905162,\n 3.4945791,\n 3.4857965,\n 3.4846985,\n 3.4836903,\n 3.4831159,\n 3.4859974,\n 3.4947174,\n 3.4826601,\n 3.4864106,\n 3.485477,\n 3.4893277,\n 3.4814088,\n 3.4872611,\n 3.4840991,\n 3.489032,\n 3.4828212,\n 3.4862323,\n 3.4812484,\n 3.4867351,\n 3.4798014,\n 3.4823449,\n 3.4857626,\n 3.4861178,\n 3.4925189,\n 3.4866865,\n 3.4830685,\n 3.4834027,\n 3.4887824,\n 3.4956322,\n 3.4827721,\n 3.5011179,\n 3.4930458,\n 3.4893074,\n 3.4880221,\n 3.4903264,\n 3.499954,\n 3.4898522,\n 3.4965441,\n 3.4918008,\n 3.4894733,\n 3.4899201,\n 3.4849873,\n 3.494235,\n 3.4954934,\n 3.491312,\n 3.4843709,\n 3.4889755,\n 3.4868069,\n 3.4955642,\n 3.5085604,\n 3.4942176,\n 3.4834304,\n 3.4860284,\n 3.4973185,\n 3.497155,\n 3.4876838,\n 3.4892476,\n 3.4921684,\n 3.4903603,\n 3.487931,\n 3.5163364,\n 3.4972816,\n 3.4938054,\n 3.4945743,\n 3.4907584,\n 3.4971778,\n 3.4926267,\n 3.4978344,\n 3.4957197,\n 3.5093834,\n 3.4974966,\n 3.5002635,\n 3.5146534,\n 3.4967163,\n 3.4985702,\n 3.5186052,\n 3.5100114,\n 3.5075994,\n 3.5048239,\n 3.5269749,\n 3.5124211,\n 3.5354133,\n 3.5154099,\n 3.5155191,\n 3.5086176,\n 3.5086277,\n 3.5071499,\n 3.5191557,\n 3.5247617,\n 3.5149806,\n 3.5350807,\n 3.5308819,\n 3.5239282,\n 3.5187955,\n 3.5331678,\n 3.5358033,\n 3.534333,\n 3.5232911,\n 3.5233948,\n 3.5187864,\n 3.5114889,\n 3.5181904,\n 3.5197079,\n 3.5411477,\n 3.5176671,\n 3.5210884,\n 3.5182581,\n 3.5298092,\n 3.5685339,\n 3.5247025,\n 3.5395098,\n 3.5442092,\n 3.5404243,\n 3.5399518,\n 3.529057,\n 3.5358164,\n 3.545851,\n 3.5716119,\n 3.552294,\n 3.547956,\n 3.5717013,\n 3.5417225,\n 3.5622785,\n 3.5440884,\n 3.5438719,\n 3.5585606,\n 3.5738437,\n 3.5657768,\n 3.5676937,\n 3.5931997,\n 3.5596857,\n 3.5782168,\n 3.5582137,\n 3.5666814,\n 3.5447497,\n 3.570183,\n 3.5647302,\n 3.5494251,\n 3.567435,\n 3.5542226,\n 3.5819964,\n 3.6162934,\n 3.6024082,\n 3.5958774,\n 3.5750568,\n 3.5764613,\n 3.6314147,\n 3.6066587,\n 3.5812221,\n 3.5831323,\n 3.6145735,\n 3.633311,\n 3.5435443,\n 3.5878654,\n 3.5934272,\n 3.5640609,\n 3.5926673,\n 3.5587707,\n 3.6266186,\n 3.5686965,\n 3.5850308,\n 3.5854762,\n 3.5615916,\n 3.5533106,\n 3.5440736,\n 3.5680757,\n 3.5781431,\n 3.6106329,\n 3.5957038,\n 3.5691597,\n 3.5897703,\n 3.6068232,\n 3.5582719,\n 3.6263456,\n 3.635901,\n 3.6165915,\n 3.6095877,\n 3.6087284,\n 3.5793438,\n 3.557889,\n 3.5954196,\n 3.5709512,\n 3.5798221,\n 3.5860062,\n 3.5610311,\n 3.5387392,\n 3.5626531,\n 3.5386136,\n 3.5909281,\n 3.5567899,\n 3.5477865,\n 3.5390587,\n 3.5464613,\n 3.5224082,\n 3.5880494,\n 3.5658169,\n 3.5382802,\n 3.5900245,\n 3.5810695,\n 3.5456522,\n 3.5717988,\n 3.5361745,\n 3.5370615,\n 3.5423663,\n 3.5860159,\n 3.5291843,\n 3.5278172,\n 3.5459464,\n 3.5371833,\n 3.5222864,\n 3.5310953,\n 3.5485923,\n 3.5504334,\n 3.520906,\n 3.538141,\n 3.5342011,\n 3.5585718,\n 3.5509703,\n 3.5312266,\n 3.5404422,\n 3.5237963,\n 3.5216818,\n 3.5239346,\n 3.523489,\n 3.5275652,\n 3.5275958,\n 3.5272532,\n 3.5268309,\n 3.551095,\n 3.5182602,\n 3.5165591,\n 3.5372777,\n 3.507103,\n 3.506597,\n 3.5332298,\n 3.5103483,\n 3.5398498,\n 3.527597,\n 3.5027902,\n 3.5260639,\n 3.542294,\n 3.52371,\n 3.5119855,\n 3.5257547,\n 3.5189393,\n 3.5161498,\n 3.5219369,\n 3.4900193,\n 3.5149503,\n 3.527988,\n 3.523241,\n 3.4988196,\n 3.5125895,\n 3.5004232,\n 3.5111082,\n 3.5417378,\n 3.5127811,\n 3.5064571,\n 3.5083895,\n 3.524219,\n 3.5504973,\n 3.5201418,\n 3.5140462,\n 3.5261629,\n 3.5094681,\n 3.5049338,\n 3.5059097,\n 3.5092843,\n 3.5023246,\n 3.509305,\n 3.4967031,\n 3.5276699,\n 3.4992504,\n 3.5028708,\n 3.497915,\n 3.5302937,\n 3.4950578,\n 3.4982178,\n 3.488965,\n 3.4999504,\n 3.5053024,\n 3.4981019,\n 3.4963083,\n 3.4982414,\n 3.4982202,\n 3.4921584,\n 3.4939797,\n 3.4980385,\n 3.4902968,\n 3.493242,\n 3.5070419,\n 3.4951735,\n 3.4952149,\n 3.4925721,\n 3.506603,\n 3.4845073,\n 3.4864371,\n 3.4839237,\n 3.4879704,\n 3.4935496,\n 3.4895811,\n 3.4959762,\n 3.483541,\n 3.4928312,\n 3.4891355,\n 3.5041866,\n 3.4916301,\n 3.4870954,\n 3.4990759,\n 3.5000601,\n 3.4987967,\n 3.5071611,\n 3.4961216,\n 3.5034037,\n 3.4908729,\n 3.4925539,\n 3.4906158,\n 3.5013909,\n 3.4880388,\n 3.4941187,\n 3.50756,\n 3.4899638,\n 3.4952223,\n 3.4894595,\n 3.4925833,\n 3.4972019,\n 3.4881339,\n 3.4937654,\n 3.4912755,\n 3.4869375,\n 3.4897308,\n 3.5015116,\n 3.5013289,\n 3.5027256,\n 3.4933078,\n 3.5114744,\n 3.5100291,\n 3.4951978,\n 3.5079553,\n 3.4981108,\n 3.4965167,\n 3.4932327,\n 3.4947345,\n 3.4935796,\n 3.4919889,\n 3.495456,\n 3.4996405,\n 3.4980052,\n 3.4947987,\n 3.497303,\n 3.5047781,\n 3.4946036,\n 3.4958911,\n 3.5031934,\n 3.4903603,\n 3.5016882,\n 3.5045981,\n 3.4974353,\n 3.5017455,\n 3.4983225,\n 3.532568,\n 3.5008309,\n 3.4985051,\n 3.5082097,\n 3.5053108,\n 3.5260603,\n 3.509433,\n 3.5169861,\n 3.5077424,\n 3.5110676,\n 3.5154197,\n 3.4959812,\n 3.5097203,\n 3.5106554,\n 3.5206058,\n 3.5067589,\n 3.5070698,\n 3.4957466,\n 3.5101807,\n 3.5069904,\n 3.5071061,\n 3.5037622,\n 3.4976869,\n 3.5029149,\n 3.5008092,\n 3.4936299,\n 3.5008099,\n 3.4877679,\n 3.4951334,\n 3.498353,\n 3.488667,\n 3.5004058,\n 3.4982028,\n 3.5089657,\n 3.4885657,\n 3.4903398,\n 3.4927249,\n 3.4809206,\n 3.482127,\n 3.4837799,\n 3.4914892,\n 3.4939392,\n 3.4898431,\n 3.485795,\n 3.481498,\n 3.4832683,\n 3.4909678,\n 3.4941645,\n 3.483423,\n 3.481282,\n 3.4899604,\n 3.4844871,\n 3.479949,\n 3.4835985,\n 3.4826343,\n 3.4877317,\n 3.4786768,\n 3.4829235,\n 3.4889042,\n 3.4827123,\n 3.4871082,\n 3.4824977,\n 3.4798796,\n 3.4803662,\n 3.4804242,\n 3.4787407,\n 3.4903128,\n 3.4754922,\n 3.4838836,\n 3.4807632,\n 3.4925728,\n 3.4781191,\n 3.4798779,\n 3.4795651,\n 3.4824166,\n 3.4751921,\n 3.4781768,\n 3.4784796,\n 3.4833937,\n 3.4843192,\n 3.4805853,\n 3.4785416,\n 3.4830725,\n 3.4838052,\n 3.4847417,\n 3.4826636,\n 3.4785337,\n 3.4852426,\n 3.4856162,\n 3.4861445,\n 3.4858918,\n 3.4844396,\n 3.4851017,\n 3.4866095,\n 3.4832103,\n 3.4814997,\n 3.4831858,\n 3.4885123,\n 3.4872825,\n 3.4845419,\n 3.483954,\n 3.4867485,\n 3.4860444,\n 3.4826438,\n 3.4838824,\n 3.4889977,\n 3.4897747,\n 3.4879787,\n 3.4882839,\n 3.4908576,\n 3.4907866,\n 3.4907162,\n 3.4849002,\n 3.4937325,\n 3.5056596,\n 3.5053113,\n 3.5036771,\n 3.5018718,\n 3.5104764,\n 3.4978776,\n 3.501626,\n 3.5059874,\n 3.5081015,\n 3.5103559,\n 3.5128052,\n 3.5137701,\n 3.5217652,\n 3.500001,\n 3.5111191,\n 3.5128379,\n 3.5226257,\n 3.5308225,\n 3.5356052,\n 3.5190663,\n 3.5191028,\n 3.5237689,\n 3.5214281,\n 3.5196607,\n 3.5185564,\n 3.518446,\n 3.5126717,\n 3.5264559,\n 3.5224824,\n 3.5459332,\n 3.5346618,\n 3.5309577,\n 3.5374348,\n 3.5239854,\n 3.5284657,\n 3.5366585,\n 3.5382273,\n 3.5307503,\n 3.5339632,\n 3.5443382,\n 3.5353525,\n 3.5199142,\n 3.5241032,\n 3.5278735,\n 3.5101643,\n 3.4986124,\n 3.512393,\n 3.5378499,\n 3.5025637,\n 3.4946368,\n 3.5005383,\n 3.4983964,\n 3.4990368,\n 3.4973533,\n 3.5071497,\n 3.4922791,\n 3.4928343,\n 3.5021245,\n 3.5019624,\n 3.4902227,\n 3.5076425,\n 3.5032473,\n 3.4936733,\n 3.4892972,\n 3.4957974,\n 3.4900053,\n 3.483362,\n 3.4918916,\n 3.4913473,\n 3.4917018,\n 3.4988873,\n 3.5033212,\n 3.4949403,\n 3.4976933,\n 3.5020456,\n 3.5127213,\n 3.5152447,\n 3.5117123,\n 3.5164342,\n 3.5208812,\n 3.5174005,\n 3.5100863,\n 3.5108075,\n 3.5167768,\n 3.5084221,\n 3.515856,\n 3.5166469,\n 3.537442,\n 3.523411,\n 3.5079482,\n 3.5244758,\n 3.5206378,\n 3.5191689,\n 3.506289,\n 3.5177624,\n 3.5229793,\n 3.5107462,\n 3.5125997,\n 3.5300832,\n 3.5060184,\n 3.5377119,\n 3.5385075,\n 3.5284982,\n 3.5294237,\n 3.5422878,\n 3.5215151,\n 3.523756,\n 3.5228639,\n 3.5360692,\n 3.5236464,\n 3.5280969,\n 3.5445197,\n 3.5206375,\n 3.5317054,\n 3.5314937,\n 3.5438368,\n 3.5179281,\n 3.5375195,\n 3.520246,\n 3.5302253,\n 3.5329247,\n 3.534225,\n 3.5510471,\n 3.5610828,\n 3.5375364,\n 3.5819979,\n 3.5682242,\n 3.5619299,\n 3.5770633,\n 3.5469811,\n 3.5449519,\n 3.5481477,\n 3.5583918,\n 3.5841978,\n 3.5808048,\n 3.560447,\n 3.5675936,\n 3.5802369,\n 3.5680163,\n 3.5502396,\n 3.5771849,\n 3.5517845,\n 3.5461307,\n 3.6063917,\n 3.5653834,\n 3.5621495,\n 3.5650387,\n 3.573926,\n 3.5479128,\n 3.5362418,\n 3.5492017,\n 3.5557067,\n 3.5772274,\n 3.5733745,\n 3.5339489,\n 3.5626774,\n 3.539849,\n 3.5590556,\n 3.58986,\n 3.5637839,\n 3.5737488,\n 3.5368176,\n 3.5396512,\n 3.5675154,\n 3.5702972,\n 3.5484817,\n 3.529676,\n 3.5721681,\n 3.5546072,\n 3.5558829,\n 3.582859,\n 3.5809286,\n 3.545506,\n 3.5608039,\n 3.5688152,\n 3.5504971,\n 3.5630057,\n 3.57517,\n 3.5645554,\n 3.5494835,\n 3.588377,\n 3.581187,\n 3.5672712,\n 3.6116457,\n 3.5761528,\n 3.5729506,\n 3.5933275,\n 3.5795217,\n 3.5854344,\n 3.5979362,\n 3.5798452,\n 3.5972295,\n 3.578316,\n 3.5564871,\n 3.614285,\n 3.5816312,\n 3.5966995,\n 3.5711803,\n 3.5877678,\n 3.5924704,\n 3.5780723,\n 3.6045012,\n 3.5831065,\n 3.5978644,\n 3.595088,\n 3.5800507,\n 3.6268148,\n 3.6250277,\n 3.5918009,\n 3.5889904,\n 3.5989423,\n 3.6076784,\n 3.6157796,\n 3.5852287,\n 3.5873394,\n 3.6421127,\n 3.5813577,\n 3.5971606,\n 3.6054173,\n 3.6021605,\n 3.546077,\n 3.5920219,\n 3.5897408,\n 3.5585291,\n 3.5665848,\n 3.5406492,\n 3.5606072,\n 3.5582294,\n 3.5485382,\n 3.5551503,\n 3.5331118,\n 3.534142,\n 3.5244238,\n 3.524215,\n 3.5075009,\n 3.5335472,\n 3.5195589,\n 3.5190299,\n 3.5486429,\n 3.5573533,\n 3.5242498,\n 3.5184321,\n 3.5123525,\n 3.507493,\n 3.553251,\n 3.5344729,\n 3.5529854,\n 3.535392,\n 3.5398099,\n 3.5474422,\n 3.5277958,\n 3.5670459,\n 3.544908,\n 3.5359154,\n 3.5749917,\n 3.5475235,\n 3.5537229,\n 3.5599074,\n 3.5875947,\n 3.5873168,\n 3.5864313,\n 3.600507,\n 3.5858729,\n 3.596194,\n 3.6003764,\n 3.5928564,\n 3.5804014,\n 3.56424,\n 3.6013281,\n 3.5628202,\n 3.5482368,\n 3.5783393,\n 3.5716097,\n 3.5987563,\n 3.5860102,\n 3.5676563,\n 3.5677829,\n 3.5858347,\n 3.5581408,\n 3.5780103,\n 3.569557,\n 3.592067,\n 3.6409018,\n 3.6203394,\n 3.6248188,\n 3.6547585,\n 3.6518588,\n 3.7226088,\n 3.6631513,\n 3.6405268,\n 3.7146866,\n 3.6796608,\n 3.7043016,\n 3.7230775,\n 3.6695223,\n 3.6742346,\n 3.6720688,\n 3.6392725,\n 3.7295439,\n 3.6629601,\n 3.5907693,\n 3.5781047,\n 3.5992386,\n 3.5749204,\n 3.5762794,\n 3.5674613,\n 3.5638468,\n 3.5309632,\n 3.581274,\n 3.562638,\n 3.5241437,\n 3.5556631,\n 3.5409596,\n 3.5530944,\n 3.5568359,\n 3.5403614,\n 3.5225248,\n 3.5436194,\n 3.5528646,\n 3.5759363,\n 3.5396774,\n 3.5503981,\n 3.5683982,\n 3.5602696,\n 3.5542018,\n 3.5545986,\n 3.5450006,\n 3.5618119,\n 3.5785017,\n 3.5699425,\n 3.5821502,\n 3.5682704,\n 3.5511568,\n 3.5659995,\n 3.5683153,\n 3.5626686,\n 3.5827751,\n 3.5909479,\n 3.5950565,\n 3.5911374,\n 3.5915864,\n 3.578464,\n 3.5865655,\n 3.6328716,\n 3.5921679,\n 3.6056211,\n 3.6150534,\n 3.6081159,\n 3.5917611,\n 3.6092446,\n 3.5887775,\n 3.6331301,\n 3.615979,\n 3.6200807,\n 3.6218758,\n 3.6137455,\n 3.6555638,\n 3.6559727,\n 3.679769,\n 3.6813641,\n 3.6357238,\n 3.6463673,\n 3.6956084,\n 3.7170854,\n 3.7289133,\n 3.7232091,\n 3.7352209,\n 3.6760678,\n 3.663573,\n 3.6447222,\n 3.6661139,\n 3.6496971,\n 3.6327145,\n 3.6167953,\n 3.620331,\n 3.6057355,\n 3.5945837,\n 3.5874858,\n 3.6123807,\n 3.585042,\n 3.5851703,\n 3.5568023,\n 3.608263,\n 3.6028137,\n 3.5785899,\n 3.6051395,\n 3.5648232,\n 3.595983,\n 3.5806696,\n 3.6112752,\n 3.5991669,\n 3.5897055,\n 3.5946388,\n 3.5994205,\n 3.5986977,\n 3.6114609,\n 3.5903537,\n 3.5589528,\n 3.604146,\n 3.5691385,\n 3.572104,\n 3.6143322,\n 3.575243,\n 3.6013086,\n 3.5875592,\n 3.5784559,\n 3.5824196,\n 3.590991,\n 3.5845537,\n 3.5809281,\n 3.5828018,\n 3.5842299,\n 3.5676658,\n 3.6009445,\n 3.5825768,\n 3.6208363,\n 3.5848699,\n 3.5803022,\n 3.5859482,\n 3.5957446,\n 3.5931003,\n 3.6211238,\n 3.5968859,\n 3.6022134,\n 3.6005101,\n 3.5746679,\n 3.5757146,\n 3.5785291,\n 3.582587,\n 3.593029,\n 3.5807292,\n 3.5950832,\n 3.5535274,\n 3.5623183,\n 3.562176,\n 3.5617223,\n 3.5603037,\n 3.5733678,\n 3.5663853,\n 3.5926971,\n 3.6136491,\n 3.6176031,\n 3.5640688,\n 3.6031058,\n 3.5805798,\n 3.5830538,\n 3.6505454,\n 3.6197195,\n 3.60673,\n 3.6688747,\n 3.6260867,\n 3.618896,\n 3.645565,\n 3.6985421,\n 3.6359153,\n 3.663249,\n 3.6414135,\n 3.6385274,\n 3.6014194,\n 3.6239154,\n 3.6957564,\n 3.6092825,\n 3.6386421,\n 3.652535,\n 3.6203051,\n 3.6508226,\n 3.6122332]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 32\n",
    "x_dim = 784\n",
    "z_dim = 100\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "d_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Net\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim], name='X')\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([x_dim, h_dim]), name='D_W1')\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]), name='D_b1')\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 1]), name='D_W2')\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]), name='D_b2')\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "# Generator Net\n",
    "Z = tf.placeholder(tf.float32, shape=[None, z_dim], name='Z')\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([z_dim, h_dim]), name='G_W1')\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[h_dim]), name='G_b1')\n",
    "G_W2 = tf.Variable(xavier_init([h_dim, x_dim]), name='G_W2')\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[x_dim]), name='G_b2')\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample, _ = log_linear(Z, G_W1, G_b1, G_W2, G_b2)\n",
    "D_real, D_logit_real = log_linear(X, D_W1, D_b1, D_W2, D_b2)\n",
    "D_fake, D_logit_fake = log_linear(G_sample, D_W1, D_b1, D_W2, D_b2)\n",
    "D_loss, G_loss = genradvers(D_real, D_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only update D(X)'s parameters, so var_list = theta_D\n",
    "D_solver = adam_optimizer(D_loss, theta_D)\n",
    "# Only update G(X)'s parameters, so var_list = theta_G\n",
    "G_solver = adam_optimizer(G_loss, theta_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    '''Uniform prior for G(Z)'''\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(1000000):\n",
    "        #  train process\n",
    "        x_mb, batch_ys = train_dataset.next_batch(mb_size, shuffle=False)\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: x_mb, Z: sample_Z(mb_size, z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, z_dim)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}